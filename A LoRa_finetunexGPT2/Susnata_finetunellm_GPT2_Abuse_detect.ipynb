{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Introduction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This project is built around the idea of using large language models to assist in decision-making for fraud detection and refund  related cases. Having worked in Amazon  I have seen how much manual effort goes into analyzing a customers issue by frontline associates checking order details, product value, history of claims, and other documentation  before deciding whether to issue a refund or request supporting documents like a police or incident report.\n",
        "\n",
        "The goal of this project was to fine-tune a pre-trained language model so that it can understand such case summaries and provide a risk-based recommendation (for example, “Low Risk,” “Medium Risk,” or “High Risk”). The model learns from real patterns of decision-making and applies those learned insights to new cases.\n",
        "\n",
        "For this purpose, I used frameworks such as Unsloth, Hugging Face Transformers, Datasets, and TRL (Transformers Reinforcement Learning). The model was fine-tuned using LoRA optimization to make the process faster and more memory efficient. The dataset was prepared and processed using pandas and loaded into a Hugging Face-compatible format for training in Google Colab.\n",
        "\n",
        "Through this project, my aim was to replicate how an experienced associate reasons through a refund decision  essentially creating a model that can act as an intelligent assistant, reducing human workload while improving consistency and speed in judgment."
      ],
      "metadata": {
        "id": "jtCKm2JIoqJp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAJDmIOdyOtd"
      },
      "outputs": [],
      "source": [
        "#importing necessary packages\n",
        "\n",
        "!pip install -q pandas tqdm datasets\n",
        "\n",
        "import pandas as pd, random, json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "aR2vPEXpy0vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"Customer_DF (1).csv\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XeLZHGvyz4Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.is_fraud.value_counts()"
      ],
      "metadata": {
        "id": "_McQupuCP6EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "7jqqP9Im2pqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Data loaded with shape:\", df.shape)\n"
      ],
      "metadata": {
        "id": "6OQYOKNT13rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will convert the above table in a json format which is required for TRL library and hugging face dataset\n",
        "\n",
        "Its kind of a prompt and response type as below"
      ],
      "metadata": {
        "id": "ncCaxvFcozv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "{\n",
        "  \"text_summary\": \"Customer from CA placed 2 orders, made 1 payment, had 2 transactions. Device: yyeiaxpltf82440jnb3v, IP: 8.129.104.40. Overall behavior appears legitimate.\",\n",
        "\n",
        "  \n",
        "  \"abuse\": \"Low risk. Approve refund normally.\"\n",
        "}"
      ],
      "metadata": {
        "id": "BqDw9sfE5ktV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename_map = {\n",
        "    'No_Transactions': 'refund_count',\n",
        "    'No_Orders': 'order_count',\n",
        "    'No_Payments': 'payment_count',\n",
        "    'Fraud': 'is_fraud'\n",
        "}\n",
        "df.rename(columns=rename_map, inplace=True)"
      ],
      "metadata": {
        "id": "oC4rMN4r5l9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# row = df.iloc[0]\n",
        "\n",
        "# row['customerDevice']"
      ],
      "metadata": {
        "id": "r-gU2IUe7ZKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering for my  LLM fine-tuning to convert to json"
      ],
      "metadata": {
        "id": "UuYI4M0xo7FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_summary(row):\n",
        "    text = (\n",
        "        f\"Customer using device {row['customerDevice']} from IP {row['customerIPAddress']} \"\n",
        "        f\"placed {row['order_count']} orders with {row['refund_count']} transactions \"\n",
        "        f\"and made {row['payment_count']} payments. \"\n",
        "        f\"Billing address: {row['customerBillingAddress']}. \"\n",
        "    )\n",
        "    # adding signals\n",
        "    if row['refund_count'] >= 3:\n",
        "        text += \"Frequent transaction activity detected. \"\n",
        "    if row['payment_count'] < row['order_count']:\n",
        "        text += \"Payment irregularities observed. \"\n",
        "    tone = \"suspicious\" if row['is_fraud'] else \"legitimate\"\n",
        "    text += f\"Overall behavior appears {tone}.\"\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "RKlmUrAy5tal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_abuse(row):\n",
        "    if row['is_fraud'] and row['refund_count'] >= 3:\n",
        "        return \"High risk refund abuse. Require Police Report.\"\n",
        "    elif row['is_fraud']:\n",
        "        return \"Medium risk refund abuse. Require Incident Report.\"\n",
        "    else:\n",
        "        return \"Low risk. Approve refund normally.\""
      ],
      "metadata": {
        "id": "AOSdn-GP7g0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm # shows progress bar\n",
        "\n",
        "tqdm.pandas()\n",
        "df[\"text_summary\"] = df.progress_apply(make_summary, axis=1)\n",
        "df[\"abuse\"] = df.progress_apply(make_abuse, axis=1)\n",
        "\n",
        "with open(\"fraud_cases.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for _, row in df.iterrows():\n",
        "        json.dump({\n",
        "            \"text_summary\": row[\"text_summary\"],\n",
        "            \"abuse\": row[\"abuse\"]\n",
        "        }, f)\n",
        "        f.write(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Fajg8NLj7xFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 3 fraud_cases.json  #fine-tuning library datasets, trl in hugging face  needs a text file"
      ],
      "metadata": {
        "id": "u1JLu6lf8tls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stage 2 Finetuning"
      ],
      "metadata": {
        "id": "ZPzyVLE99ckM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "KTC5mRck-Jye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from unsloth import is_bfloat16_supported   #brain float"
      ],
      "metadata": {
        "id": "VR3MNP25_6z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Load dataset (JSON format)\n",
        "dataset = load_dataset(\"json\", data_files=\"fraud_cases.json\", split=\"train\")\n",
        "print(\"Dataset example:\", dataset[0])"
      ],
      "metadata": {
        "id": "tzHWmpLLAr-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer & base model\n",
        "MODEL_NAME = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token  #sequences = same length gpt 2 cannot handle automatically it was trained as a causal LM (predict next token),\n",
        "# that is predict next token given all other token"
      ],
      "metadata": {
        "id": "qRYQfGWIBfj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    load_in_8bit=True,  # efficient mixed-precision loading\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#gpt 2 medium 355 million parameters"
      ],
      "metadata": {
        "id": "S9rZ_zccGLj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "print(\" Model and tokenizer loaded in 8-bit mode successfully.\")"
      ],
      "metadata": {
        "id": "O0Q5PJmtHa2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config=model.config\n",
        "print(config)"
      ],
      "metadata": {
        "id": "zAcbHBb8pJYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n"
      ],
      "metadata": {
        "id": "-ntfBSPYHtyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT 2 Medium has 24 transformer blocks nx layers , each with 16 self-attention heads. Each head operates over a 64-dimensional subspace, giving a hidden size of 1024. The total parameter count 345M  comes from attention weights, feed-forward projections, and embeddings\n"
      ],
      "metadata": {
        "id": "gqsrWsNRpHex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I target the three core Linear layers:\n",
        "#   c_attn → combined Q, K, V projection\n",
        "#   c_fc   → MLP expansion\n",
        "#   c_proj → output projection (both in attention & MLP)"
      ],
      "metadata": {
        "id": "LuUhy5CBI8_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r = 8,#rank hyperparamter\n",
        "    lora_alpha = 32, # a higher alpha value will assign more weight to the LoRA activations  alpha/rank which is delata  a scaling paramter i.e delta r\n",
        "    target_modules = [\"c_attn\", \"c_fc\", \"c_proj\"],  # my traget modules in transformer architecture\n",
        "    lora_dropout = 0.05, # dropout regulirization\n",
        "    bias = \"none\",#I can also keep it all or lora_only\n",
        "    task_type = \"CAUSAL_LM\"# GPt 2 type\n",
        ")\n"
      ],
      "metadata": {
        "id": "PHnnEyvxGCXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "print(\" LoRA adapters attached successfully.\")\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "dE3gyhnGH1ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Format each case & response pair and tokenize.\"\"\"\n",
        "    merged_texts = []\n",
        "    for summary, abuse in zip(examples[\"text_summary\"], examples[\"abuse\"]):\n",
        "        text = f\"### Case:\\n{summary}\\n\\n### Response:\\n{abuse}\"\n",
        "        merged_texts.append(text)\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        merged_texts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024,\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "print(\" Tokenization complete.\")\n",
        "print(\"Sample tokens:\\n\", tokenizer.decode(tokenized_dataset[0][\"input_ids\"][:200]))# i am seeing for sanity first 200 tokens"
      ],
      "metadata": {
        "id": "k3_c1WemI4Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_dataset,# my data set\n",
        "    dataset_text_field=\"text_summary\",\n",
        "    max_seq_length=1024,\n",
        "    packing=False,# It can make training 5x faster for short sequences. concatenate short sequences into single examplesand often improves throughput\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./fraud_investigator_model\",\n",
        "        per_device_train_batch_size=2, # The batch size per GPU/TPU core\n",
        "        gradient_accumulation_steps=4,# Number of steps to perform befor each gradient accumulation\n",
        "\n",
        "        # So my effective batch size = 8 (per step)\n",
        "        learning_rate=2e-4,# controlling gradient update step\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        max_steps=100, #Stopping training after these many steps.\n",
        "        warmup_steps=5, #Number of steps during which LR linearly increases from 0  LR valuePrevents early overshooting.\n",
        "\n",
        "\n",
        "        logging_steps=5,# Printting  metrics (loss, accuracy, etc.) every 5 steps.\n",
        "        optim=\"adamw_8bit\", # Optimizer adam\n",
        "        weight_decay=0.01,# L2 regulirazrion discourages huge weights\n",
        "        lr_scheduler_type=\"linear\", #learning rate scheduler controls how the learning rate changes during training.\n",
        "\n",
        "        #types like \"constant\",   \"linear\"  , polynomial\n",
        "        # Linear is like  Starts high and then  linearly decreases to 0\n",
        "        report_to=\"none\",  # can be used  for obervability in tensorboard\n",
        "        seed=1997, #for reproducibilty\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2\n",
        "    ),\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "l0SVFhgJI4kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()\n",
        "final_loss = trainer_stats.training_loss\n",
        "perplexity = math.exp(final_loss)\n",
        "\n",
        "print(f\"Training complete — Final Loss: {final_loss:.4f}, Perplexity: {perplexity:.4f}\")\n"
      ],
      "metadata": {
        "id": "vWBlVhfPJKI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#My Training is  complete the Final Loss: 0.7582, Perplexity: 2.1344"
      ],
      "metadata": {
        "id": "P8FSKuUtpdk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#training_loss is the average negative log-likelihood per token across training steps.\n",
        "\n",
        "#perplexity = math.exp(final_loss) gives intuition\n",
        "\n",
        "\n",
        "# A perplexity of  around 2.1344  means that, on average, the model is choosing between fewer than 2 plausible tokens at each step → showing high confidence.\n",
        "\n",
        "# Unlike regression tasks, the goal here isn’t driving loss toward zero, but ensuring a steady downward trend in loss and perplexity, paired with qualitative improvements in reasoning outputs."
      ],
      "metadata": {
        "id": "VZ0A6kZupjOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model in my drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copying  the  final fine-tuned model folder to my  Google Drive\n",
        "!cp -r ./fraud_investigator_model /content/drive/MyDrive/\n",
        "print(\" Fine-tuned model successfully saved to Google Drive at:\")\n",
        "print(\"/content/drive/MyDrive/fraud_investigator_model/final_model\")"
      ],
      "metadata": {
        "id": "yVV2b8fwK_uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# later on if i plan to do inference\n",
        "# Mounting my  Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# #Loading  model & tokenizer from Drive\n",
        "# model_path = \"/content/drive/MyDrive/fraud_investigator_model\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# #  Creating  pipeline for inference\n",
        "# fraud_detector = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "JWXtdmFvqyG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# doing a little inference to check it from my previous pipeline that was trained\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"./fraud_investigator_model/final_model\",\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "prompt = \"\"\"### Case:\n",
        "Customer using device z8x31pq from IP 192.55.44.10 has placed 9 orders in the past 10 days.\n",
        "Out of these, 1 were refunded citing “item not received”.\n",
        "Multiple accounts share the same billing address: 44 Maple Street, San Diego, CA.\n",
        "Payment method changed twice in one week.\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "output = pipe(prompt, max_new_tokens=150, temperature=1.2)# i can twek temp to have more detailed output\n",
        "print(\" Model Prediction:\\n\", output[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "zYywoL_dLFEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " Model Prediction:\n",
        " ### Case:\n",
        "Customer using device z8x31pq from IP 192.55.44.10 has placed 9 orders in the past 10 days.\n",
        "Out of these, 1 were refunded citing “item not received”.\n",
        "Multiple accounts share the same billing address: 44 Maple Street, San Diego, CA.\n",
        "Payment method changed twice in one week.\n",
        "\n",
        "### Response:\n",
        "Low risk. Approve refund with issue noted."
      ],
      "metadata": {
        "id": "-JgIW9kTsVLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My conclusion\n",
        "\n",
        "This fine-tuning project demonstrates how a domain-specific language model can learn the reasoning patterns behind real-world decision-making processes. The model acts as a foundation for what could evolve into a more advanced, agentic system — one that not only interprets cases but also takes action based on learned logic.\n",
        "\n",
        "I may give it access to tools and crm to update dashboard too\n",
        "\n",
        "In the future, this concept can be extended by integrating the fine-tuned model with APIs or backend systems to create a human-in-the-loop agent. Such an agent could automatically read case descriptions, evaluate them, and perform tasks like issuing refunds, denying requests, or requesting documents, while still allowing a human reviewer to approve or override the decision.This make the concept Human in the loop\n",
        "\n",
        "The broader vision is to move toward automation — where language models do not just generate responses but assist in real operational workflows with accountability, speed, and accuracy. This project serves as a first step toward that goal, blending human understanding with machine intelligence to make front facing processes smarter and more efficient."
      ],
      "metadata": {
        "id": "zUWr2k6vp0Mr"
      }
    }
  ]
}