{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf8vN11Kjsj4"
      },
      "outputs": [],
      "source": [
        "!pip install -q unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "BMpAP_majwf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048 # choosing inputs max tokens  context window llama 128k, llama3.2's embedding size is 3072 first input layer also 3072,  ffn expansion 2.7x 8192 ,attention head 24in number. each learns its qkv and attention score , each dimension by attention head 128,gqa,  kv head 8\n",
        "dtype = None # None for auto detection.\n",
        "load_in_4bit = True # Using 4bit quantization to reduce memory usage.I can put it to false also\n"
      ],
      "metadata": {
        "id": "I5XQ4oCJkJE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model\n",
        ")"
      ],
      "metadata": {
        "id": "SSjdx3ClmevU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#getting peft and defining paramweters for LORA\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, #Hyperparameter\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], #llama has a exclusive gate proj  eg  , wq plus delta w adapter\n",
        "    #x>>W1up, w2up, >>xw1up, activations (xw2up),>>>w down\n",
        "\n",
        "\n",
        "   lora_alpha = 16, # a higher alpha value will assign more weight to the LoRA activations  alpha/rank which is delata  a scaling paramter\n",
        "   lora_dropout = 0, #dropout regularization\n",
        "    bias = \"none\",    #I can also keep it all or lora_only\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 1997,\n",
        "    use_rslora = False,# my rank stabilised lora in case lora alpha is unstable\n",
        "    loftq_config = None#lora and qlora fusion wehn i am quantizing its learn low rank adaption simuletaneously for better accuracy\n",
        ")"
      ],
      "metadata": {
        "id": "qvuA-YFNmt37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\",'v0', split = \"train\")\n",
        "\n",
        "#https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT"
      ],
      "metadata": {
        "id": "BOkugY6Potj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[:5])"
      ],
      "metadata": {
        "id": "K5WVSQXZot7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now creating a prompt  that i will  use to finetune our Llama model\n",
        "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
        "<problem>\n",
        "{}\n",
        "</problem>\n",
        "\n",
        "{}\n",
        "{}\n",
        "\"\"\"\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "  problems = examples[\"problem\"]\n",
        "  thoughts = examples[\"reannotated_assistant_content\"]\n",
        "  solutions = examples[\"solution\"]\n",
        "  texts = []\n",
        "\n",
        "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
        "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
        "    texts.append(text)\n",
        "\n",
        "  return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CuJ3pwxQ5PPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a trainer object from transformer reinforcement library from hugging face\n",
        "\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,  #that LoRA model\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2, # Number of processors to use for processing the dataset\n",
        "    packing = False, # It can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2, # The batch size per GPU/TPU core\n",
        "        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation\n",
        "        warmup_steps = 5, # Few updates with low learning rate before actual training\n",
        "        max_steps = 60, # Specifies the total number of training steps (batches) to run.\n",
        "        learning_rate = 2e-4,# controlling gradient update step\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\", # Optimizer\n",
        "        weight_decay = 0.01,# L2 regulirazrion discourages huge weights\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",#directory where it will be stored\n",
        "        report_to = \"none\", # can be used  for obervability in tensorboard\n",
        "    ),\n",
        ")\n",
        "\n",
        "#2x4x60 480 examples\n",
        "#low enropy model better ate predicting next token"
      ],
      "metadata": {
        "id": "TgywfUHt8UOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "CXeuZQMVDSVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "final_loss = trainer_stats.training_loss  # average loss across training\n",
        "perplexity = math.exp(final_loss)\n",
        "print(f\"Final loss: {final_loss:.4f}, Perplexity: {perplexity:.4f}\")"
      ],
      "metadata": {
        "id": "WzricdAvHozL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MY TAKEAWAYS\n",
        "\n",
        "\n",
        "# In LLM fine-tuning, the training objective is next-token prediction, not numeric regression.\n",
        "\n",
        "# Therefore, loss values don’t approach zero( close to 0.5 is good enough depends on guidelines) — instead, they reflect how well the model predicts tokens from a large vocabulary.\n",
        "\n",
        "# In my run, the model achieved:\n",
        "\n",
        "# Final Loss: ~0.6264\n",
        "\n",
        "# Perplexity : ~1.87\n",
        "\n",
        "# A loss of ~0.6 indicates the model is learning the dataset’s reasoning patterns effectively.\n",
        "\n",
        "# A perplexity of ~1.87 means that, on average, the model is choosing between fewer than 2 plausible tokens at each step → showing high confidence.\n",
        "\n",
        "# Unlike regression tasks, the goal here isn’t driving loss toward zero, but ensuring a steady downward trend in loss and perplexity, paired with qualitative improvements in reasoning outputs."
      ],
      "metadata": {
        "id": "mkyHmjTEIIza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INFERENCE"
      ],
      "metadata": {
        "id": "aclhuKLBLsZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking. Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer\n",
        "\n",
        "{problem}\n",
        "\"\"\"\n",
        "\n",
        "message = sys_prompt.format(problem=\"If its raining why is the sky grey\")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")\n",
        "FastLanguageModel.for_inference(model) # Enables 2x faster inference\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": message},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,\n",
        "                         temperature = 1.5, min_p = 0.1)\n",
        "response = tokenizer.batch_decode(outputs)  #batch decode helps me to convert ids back into natural language so that i can see"
      ],
      "metadata": {
        "id": "NdSmvLhEK_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0])"
      ],
      "metadata": {
        "id": "k427T1DrLjZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.batch_decode(outputs,skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "K3DEKiDoPgCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0])"
      ],
      "metadata": {
        "id": "EgAUFYvyPu6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving final model weights LORA  and tokenizer\n",
        "trainer.save_model(\"outputs/final_model\")\n",
        "tokenizer.save_pretrained(\"outputs/final_model\")"
      ],
      "metadata": {
        "id": "0NMmoaSDQbxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PrNp-ighQiER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/outputs/final_model /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "id": "UPGGMgSmQmAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/final_model"
      ],
      "metadata": {
        "id": "e6NRcVlsRMAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VhBTfFuHSF5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bcHD73rQSEyI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}