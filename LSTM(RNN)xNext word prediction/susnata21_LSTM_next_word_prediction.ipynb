{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " PROJECT INTRODUCTION\n",
        "\n",
        "This project builds a Next-Word Prediction model using a Word-Level LSTM trained on the classic Tiny Shakespeare dataset.\n",
        "\n",
        "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "LSTMs were once state-of-the-art for sequence modeling. They capture temporal dependencies using hidden states (ht, ct), but they struggle with:\n",
        "• Long-range dependencies\n",
        "• Large vocabularies\n",
        "• Slow training due to sequential processing\n",
        "• Vanishing or exploding gradients\n",
        "\n",
        "The goal of this project is:\n",
        "1. Build a simple LSTM-based language model\n",
        "2. Evaluate it using Loss, Accuracy, and Perplexity\n",
        "3. Understand its limitations\n",
        "4. Motivate why modern architectures like Transformers perform far better\n"
      ],
      "metadata": {
        "id": "oAMKldwpx7Yf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZSeu-EFf1Yd8"
      },
      "outputs": [],
      "source": [
        "#Installing necessary libraries and packages\n",
        "\n",
        "! pip install tensorflow numpy requests matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests # fetching the corpus\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "1dXFjkjq1jTL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining my parameters"
      ],
      "metadata": {
        "id": "IbkefiGSJLDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CORPUS_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"  #  corpus\n",
        "LOWERCASE = True\n",
        "\n",
        "VOCAB_SIZE_CAP = 2000   # i am picking 5000 most frequent words from Alice in wonderland and rare words are mapped to OOV ,output layer is dense 5000 with softmax\n",
        "SEQ_LEN =40           #  context window size how many past words to condition ontimestamp, ht ct ,output t -50 #lstm struggle with long sequences\n",
        "EMBED_DIM = 256         # size of learned word vectors embedding dense continous not discete types which is static\n",
        "LSTM_UNITS = 128\n",
        "  # hidden units or neurons  in the LSTM cell\n",
        "DROPOUT = 0.3        # Dropoout regularization to reduce overfitting\n",
        "LR = 0.0005           # initial learning rate for Adam optizer so that it does not overshoot did some hit and try\n",
        "EPOCHS = 30\n",
        "VAL_SPLIT = 0.1  # validation split of data\n",
        "BATCH_SIZE = 32    #batch size\n",
        "TOP_K = 5                # how many top candidates to show during inference#i can also use min p or top p nucleus"
      ],
      "metadata": {
        "id": "MolnYaxz1jb9"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load & cleaning  corpus\n",
        "def fetch_corpus(url):\n",
        "    r = requests.get(url, timeout=30)# raising error if something goes wrong\n",
        "    r.raise_for_status()\n",
        "    return r.text\n",
        "\n",
        "raw_text = fetch_corpus(CORPUS_URL)\n",
        "\n"
      ],
      "metadata": {
        "id": "0jHVpPXg1jib"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning text\n",
        "\n",
        "def clean(text, lowercase=True):\n",
        "    if lowercase:\n",
        "        text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z'.,!?;:\\-\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "text = clean(raw_text)"
      ],
      "metadata": {
        "id": "X0Pq7sA-s7sb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE_CAP, oov_token=\"<OOV>\") # 2000 as defined above and others as oov\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "word_index = tokenizer.word_index # crteates word id mapping\n",
        "vocab_size = min(VOCAB_SIZE_CAP, len(word_index) + 1) if VOCAB_SIZE_CAP else (len(word_index) + 1) #plus 1 for padding reserved at 0 position\n",
        "\n",
        "# Converting  full text to token IDs\n",
        "tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Create sliding windows of length SEQ_LEN+1( mine is defined as  40   )    # first SEQ_LEN are inputs, last one is the target word\n",
        "sequences = []\n",
        "for i in range(SEQ_LEN, len(tokens)):\n",
        "    seq = tokens[i-SEQ_LEN:i+1]\n",
        "    sequences.append(seq)\n",
        "\n",
        "sequences = np.array(sequences, dtype=np.int32)\n",
        "\n",
        "X = sequences[:, :-1]   # inputs of length SEQ_LEN\n",
        "y = sequences[:, -1]    # next-word labels"
      ],
      "metadata": {
        "id": "pqo_aNf81jxS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding uses a single special token (0), but this token can be repeated multiple times to make all sequences match a fixed length. The number of padding positions varies for each sequence, but the padding token is always the same."
      ],
      "metadata": {
        "id": "QPUTXYUCWk2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5bH6v4oZU5t",
        "outputId": "4301e398-d603-4066-b2a0-86ded0f36d05"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(204022, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1luGzJWOQUuk",
        "outputId": "d5af2a9d-9bb5-4884-9926-731fdf32d970"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrFpamkQLnv4",
        "outputId": "6d17a461-9459-4360-a482-c6bb0c95ebc2"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  89,  270,  140,   36,  969,  144,  669,  128,   16,  103,   34,\n",
              "        103,  103,   89,  270,    7,   41,   34, 1268,  351,    4,  200,\n",
              "         64,    4,    1,   34, 1268, 1268,   89,  270,   89,    7,   92,\n",
              "       1141,  232,   12,    1,  581,    4,    2], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0][-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8pxNGcYTH7L",
        "outputId": "7a789356-c732-493c-a4ad-056113c05de4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int32(2)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCsD9t2df8wB",
        "outputId": "319adeb4-a27e-49ef-b605-54459ff029a1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(306)"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets see 2nd sample"
      ],
      "metadata": {
        "id": "KdwDeiD6S547"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[1] #2nd sample last one is 74   which is target for X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZeLmHbKSv3i",
        "outputId": "6de2c6dc-b89e-4c50-a416-7a048667ccc9"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 270,  140,   36,  969,  144,  669,  128,   16,  103,   34,  103,\n",
              "        103,   89,  270,    7,   41,   34, 1268,  351,    4,  200,   64,\n",
              "          4,    1,   34, 1268, 1268,   89,  270,   89,    7,   92, 1141,\n",
              "        232,   12,    1,  581,    4,    2,  306], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In next-word prediction, each training sample is created using a sliding window over the text. For a sentence like “my name is Susnata”, the model sees partial sequences as inputs and learns to predict the next word. For example, the first input sequence is [\"my\"] and its target is \"name\". The second input is [\"my\", \"name\"] and its target is \"is\". The third input is [\"my\", \"name\", \"is\"], and its target is \"Susnata\". In general, each X[i] contains all words except the last one in that window, while Y[i] is the next word that naturally follows in the text. Because the window shifts by one position, the target of X[i] becomes the last element of X[i+1]. This is the basis of how sequence data is prepared for LSTM next-word prediction."
      ],
      "metadata": {
        "id": "PRoqA7AmUV7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reverse mapping for testing\n",
        "\n",
        "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "slavJh1wOTOJ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index.get(1225, \"<OOV>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hL_W25FkOY7O",
        "outputId": "15aef30e-576c-4135-bde6-c5155d3460ae"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'condition'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index.get(74, \"<OOV>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ehDTo7ylObvU",
        "outputId": "f9f7d79c-aae9-437d-cd6b-e2deaf47233a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i'll\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB7qzvB-PzPg",
        "outputId": "d4e3d9f7-8325-459d-f8b9-fa28dddd6839"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  One-hot encode targets & pad inputs at pre\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "max_len = X.shape[1]  # should be  equal SEQ_LEN 21\n",
        "X = pad_sequences(X, maxlen=max_len, padding='pre')  # safety, already fixed-length to 210"
      ],
      "metadata": {
        "id": "hQ1CK7tg8WTz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X is padded to a fixed sequence length using the PAD token (0). Y is a single integer representing the next word, and after one-hot encoding it becomes a vector of size vocab_size (not the sequence length). Padding is only for input sequences, not for targets."
      ],
      "metadata": {
        "id": "Db9yoggIYa0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization"
      ],
      "metadata": {
        "id": "QsD91cAjqCrO"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  6) Build the model with hyperparameters defined earlier\n",
        "model = Sequential([\n",
        "    Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=EMBED_DIM,\n",
        "    input_length=max_len,\n",
        "    embeddings_regularizer=tf.keras.regularizers.l2(1e-6)  # tiny l2 regulirization\n",
        "),\n",
        "    LayerNormalization(),\n",
        " LSTM(LSTM_UNITS),\n",
        "    Dropout(DROPOUT),\n",
        "\n",
        "    Dense(256, activation=\"relu\"),\n",
        "    Dropout(0.3),\n",
        "\n",
        "\n",
        "\n",
        "    Dense(vocab_size, activation=\"softmax\")  #soft max as multi class classification #vocab size is 2000\n",
        "])\n",
        "\n",
        "opt = Adam(learning_rate=LR, clipnorm=1.0) # also adding gradient clipping for exploding gradient\n",
        "\n",
        "model.compile(\n",
        "    loss=\"categorical_crossentropy\",#as one hot encoded\n",
        "    optimizer=opt,\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "yMLTN9ET8ZAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "166531c1-ccd0-41a7-c5ea-6b3d32e81648"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_3           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_3           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=32"
      ],
      "metadata": {
        "id": "hH2eSMYFZCyP"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReduceLROnPlateau automatically slows down the learning rate when your validation loss stops improving. If the model doesn’t get better for a few epochs, it reduces the LR so training can continue more smoothly and avoid getting stuck. It basically tells the optimizer to take smaller steps now"
      ],
      "metadata": {
        "id": "v8pOF-rnmHR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True),#early stopping if overfitting with patience\n",
        "    ReduceLROnPlateau(\n",
        "    monitor=\"val_loss\",\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X, y,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcRD46kX8bXH",
        "outputId": "39026bad-e951-4d1c-e23d-027208a2ab63"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 10ms/step - accuracy: 0.1198 - loss: 5.6809 - val_accuracy: 0.1429 - val_loss: 5.4311 - learning_rate: 5.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.1451 - loss: 5.1235 - val_accuracy: 0.1505 - val_loss: 5.3711 - learning_rate: 5.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 11ms/step - accuracy: 0.1521 - loss: 4.9622 - val_accuracy: 0.1538 - val_loss: 5.3786 - learning_rate: 5.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 11ms/step - accuracy: 0.1570 - loss: 4.8667 - val_accuracy: 0.1535 - val_loss: 5.3766 - learning_rate: 5.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.1601 - loss: 4.7914 - val_accuracy: 0.1553 - val_loss: 5.3933 - learning_rate: 5.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.1652 - loss: 4.7062 - val_accuracy: 0.1563 - val_loss: 5.3986 - learning_rate: 2.5000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 11ms/step - accuracy: 0.1682 - loss: 4.6453 - val_accuracy: 0.1550 - val_loss: 5.4161 - learning_rate: 2.5000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 10ms/step - accuracy: 0.1698 - loss: 4.6095 - val_accuracy: 0.1555 - val_loss: 5.4395 - learning_rate: 2.5000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 10ms/step - accuracy: 0.1735 - loss: 4.5561 - val_accuracy: 0.1559 - val_loss: 5.4584 - learning_rate: 1.2500e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 11ms/step - accuracy: 0.1735 - loss: 4.5304 - val_accuracy: 0.1551 - val_loss: 5.4739 - learning_rate: 1.2500e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 11ms/step - accuracy: 0.1764 - loss: 4.4932 - val_accuracy: 0.1552 - val_loss: 5.4998 - learning_rate: 1.2500e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m5739/5739\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 11ms/step - accuracy: 0.1786 - loss: 4.4734 - val_accuracy: 0.1556 - val_loss: 5.5049 - learning_rate: 6.2500e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_train_loss = history.history[\"loss\"][-1]\n",
        "final_val_loss = history.history[\"val_loss\"][-1]\n",
        "final_train_acc = history.history[\"accuracy\"][-1]\n",
        "final_val_acc = history.history[\"val_accuracy\"][-1]"
      ],
      "metadata": {
        "id": "W-RrQV9NyqxB"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_val_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxOudah3ytKb",
        "outputId": "6fa8efff-02e8-42cb-ee6f-2ac0c227c9ae"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.50492000579834"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#  PERPLEXITY = exp(loss)\n",
        "def susnata_compute_perplexity(model, X_val, y_val):\n",
        "    loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
        "    ppl = np.exp(loss)\n",
        "    print(f\"Validation Loss: {loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "    print(f\"Validation Perplexity: {ppl:.2f}\")\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "M8I71HuoyJy7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "susnata_compute_perplexity(model, X_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J95X9lAMyO64",
        "outputId": "028317a5-2c05-493b-e59e-1325512640bb"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 5.3710\n",
            "Validation Accuracy: 0.1505\n",
            "Validation Perplexity: 215.09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(215.08795122807007)"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity measures how many equally-plausible next words the model is juggling; higher perplexity means the model is confused between many possible tokens."
      ],
      "metadata": {
        "id": "F84xMcIizBEg"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cHaMSHmOyOES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Doing inference setup with top k as 5 ,i can use top p nucleus or min p as per business requirement\n",
        "\n",
        "def susnata_predict_next_word(seed_text, top_k=5):\n",
        "    # preprocess\n",
        "    seed_text = seed_text.lower().strip()\n",
        "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    seq = seq[-SEQ_LEN:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQ_LEN, padding='pre')\n",
        "\n",
        "    # model prediction\n",
        "    probs = model.predict(seq, verbose=0)[0]\n",
        "\n",
        "    # get top-k indices sorted by probability\n",
        "    top_ids = np.argsort(probs)[-top_k:][::-1]\n",
        "\n",
        "    # build list of (word, probability)\n",
        "    results = []\n",
        "    for idx in top_ids:\n",
        "        word = reverse_word_index.get(idx, \"<OOV>\")\n",
        "        prob = float(probs[idx])\n",
        "        results.append((word, prob))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ujhfRphn0_1d"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(seed_text, num_words=10, greedy=True, top_k=5):\n",
        "    text = seed_text.strip()\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        seq = tokenizer.texts_to_sequences([text.lower()])[0]\n",
        "        seq = seq[-SEQ_LEN:]\n",
        "        seq = pad_sequences([seq], maxlen=SEQ_LEN, padding='pre')\n",
        "\n",
        "        probs = model.predict(seq, verbose=0)[0]\n",
        "\n",
        "        if greedy:\n",
        "            next_id = np.argmax(probs)\n",
        "        else:\n",
        "            # top-k sampling\n",
        "            top_ids = np.argsort(probs)[-top_k:]\n",
        "            top_probs = probs[top_ids]\n",
        "            top_probs = top_probs / top_probs.sum()\n",
        "            next_id = np.random.choice(top_ids, p=top_probs)\n",
        "\n",
        "        next_word = reverse_word_index.get(next_id, \"<OOV>\")\n",
        "        text += \" \" + next_word\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "NFqgTCXO1JBS"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(seed_text, top_k=5):\n",
        "    # preprocess\n",
        "    seed_text = seed_text.lower().strip()\n",
        "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    seq = seq[-SEQ_LEN:]\n",
        "    seq = pad_sequences([seq], maxlen=SEQ_LEN, padding='pre')\n",
        "\n",
        "    # model prediction\n",
        "    probs = model.predict(seq, verbose=0)[0]\n",
        "\n",
        "    # get top-k indices sorted by probability\n",
        "    top_ids = np.argsort(probs)[-top_k:][::-1]\n",
        "\n",
        "    # build list of (word, probability)\n",
        "    results = []\n",
        "    for idx in top_ids:\n",
        "        word = reverse_word_index.get(idx, \"<OOV>\")\n",
        "        prob = float(probs[idx])\n",
        "        results.append((word, prob))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "1tmTZ6vo1d0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Demo\n",
        "seed = \"The fruits of love I mean, my \"\n",
        "print(\"Seed:\", seed)\n",
        "print(\"Top candidates:\",susnata_ predict_next_word(seed, top_k=TOP_K))\n",
        "print(\"Greedy generation:\", generate_text(seed, num_words=10, greedy=True))\n",
        "print(\"Top-k sampled generation:\", generate_text(seed, num_words=5, greedy=False, top_k=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHm3zKdD8x7l",
        "outputId": "85ee2dae-c81c-4840-8ef6-b6377d732001"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: The fruits of love I mean, my \n",
            "Top candidates: [('<OOV>', 0.05793144181370735), ('lord', 0.039422597736120224), ('son', 0.013679040595889091), ('heart', 0.012700056657195091), ('brother', 0.012518608942627907)]\n",
            "Greedy generation: The fruits of love I mean, my  <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "Top-k sampled generation: The fruits of love I mean, my  heart to <OOV> the <OOV>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"susnata_lstm_nextword_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCnFOfV59BRw",
        "outputId": "f72aaeb2-1f9d-4dd3-86c4-eeef53c996b3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  from google.colab import files\n",
        "#  files.download(\"susnata_lstm_nextword_model.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tJsFLZxa-AlR",
        "outputId": "53d00e23-53a2-4851-f111-d968b269bea9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_40a391d3-2f07-431a-82c5-4b6c3da83044\", \"susnata_lstm_nextword_model.h5\", 15128120)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # choose the H5 file"
      ],
      "metadata": {
        "id": "hp2wgSON-IiR"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "# model = load_model(\"lstm_nextword_model.h5\")"
      ],
      "metadata": {
        "id": "E2cm_kUP-MnJ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My CONCLUSION\n",
        "\n",
        "This project built a word-level LSTM language model on the Tiny Shakespeare dataset, evaluated its loss, accuracy, and perplexity, and demonstrated how classical recurrent networks struggle with language modeling. While the LSTM learned short patterns, it failed to capture long-range structure, showed high perplexity, and plateaued quickly due to sequential bottlenecks, limited context memory, and difficulty modeling large vocabularies.\n",
        "\n",
        "Modern architectures like Transformers outperform LSTMs because they use self-attention instead of recurrence, process all tokens in parallel, capture long dependencies directly, scale to much larger models, and train more efficiently. As a result, contemporary NLP systems (GPT, BERT, Llama, etc.) achieve dramatically lower perplexity and superior language understanding compared to LSTMs.\n",
        "\n",
        "This project shows the historical value of LSTMs but highlights why modern attention-based models dominate today’s AI landscape."
      ],
      "metadata": {
        "id": "dTxCwTGszOKF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12uMAVhazN74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}